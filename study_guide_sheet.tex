\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}


\pdfinfo{
  /Title (example.pdf)
  /Creator (TeX)
  /Producer (pdfTeX 1.40.0)
  /Author (Seamus)
  /Subject (Example)
  /Keywords (pdflatex, latex,pdftex,tex)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.25in,left=.25in,right=.25in,bottom=.25in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

{\scriptsize
\hspace{5pt} $\cdot$ {\bf Capacity Factor} - the ratio of the actual output of a power plant over a given period of time to its potential output if it were to operate at maximum capacity over that same time interval. Capacity factor describes to what degree a power plant operates (how often or at what output level).\\{\bf Total Energy = PowerOutput(Nameplate Capacity) $\times$ Time $\times$ CapacityFactor

\hspace{5pt} $\cdot$ {\bf 1$^{st}$ Law Efficiency} - the ratio of useful work produced (electricity, or electricity and purposeful heating in the case of cogeneration) to the amount of primary energy put in to the power plant to produce that useful work. Efficiency describes how well a power plant converts energy from one form into another.\\}

\hspace{5pt} $\cdot$ {\bf High Voltage Transmission} - high voltage is used for transmission of electricity because P$_{loss} = I^{2}\times$R, which means that line losses increase at the current over the line goes up. Thus, we’d like to reduce current to reduce line loses. Power = Voltage x Current, meaning that if power flow is fixed, there is an inverse relationship between voltage and current. If we want a smaller current value at a fixed power flow, then the voltage has to increase. In sum, high voltage means low current, which in turn means smaller transmission loses.

\hspace{5pt} $\cdot$ {\bf Benefits/Pitfalls of using Biofuels for Transportation} - {\it potential benefits:} Lower lifecycle carbon emissions (cellulosic ethanol), reduced dependence on foreign oil (US context), liquid fuels can work with existing vehicles and fueling infrastructure.\\{\it potential pitfalls:} Competition between food and fuel (increased food prices), indirect land use change, depletion of soil nutrients or increased use of artificial fertilizers

\hspace{5pt} $\cdot$ {\bf Electricity industry's development from Edison’s small decentralized utility model to one of large centralized power stations} - Scales of economy and improving efficiencies meant that electricity could be produced more cheaply by larger turbines.The development of alternating current allowed electricity to be transmitted over longer distances with less loss.

\hspace{5pt} $\cdot$ {\bf Nuclear Power Plants Pros/Cons} - Potential benefits: Low lifecycle carbon emissions, a baseload generator (replacement for coal), domestic supply of uraniumCons: Environmental damage from mining uranium, uncertainties about cost, nuclear waste disposal (including nuclear weapons proliferation), concerns about safety.\\
\subsubsection*{}
\hspace{5pt} $\cdot$ {\bf Future Value} = Present Value (PV) $\times$ (1 + r)$^{n}$\\
\hspace{5pt} $\cdot$ {\bf Uniform Payments} = PV $\times$ ($\frac{r}{1-(1+r)^{-n}}$) [$\leftarrow${\it capital recovery factor}] \\{\it Relates a series of n uniform payment each equivalent to U over given time period to a present value P at interest rate r. Use this to determine the annual payments on a loan.}\\
\hspace{5pt} $\cdot$ {\bf Capital Recovery Value (CRF)} = ($\frac{r}{1-(1+r)^{-n}}$)\\
\hspace{5pt} $\cdot$ {\bf Net Present Value} = $\sum_{0}^{n}\frac{B_{t}-C_{t}}{(1+r)^{t}}$\\
{\it The present value of the sum of all expected future cost (Ct) and benefits (Bt) assessed at regular intervals (t) for a time period (n)}\\
\hspace{5pt} $\cdot$ {\bf Total Consumption over t years} = (annual consumption)($\frac{e^{rt}}{r} - \frac{1}{r})$, r is annual consumption growth rate \& t is consumption number of years\\
\hspace{5pt} $\cdot$ {\bf Efficiency} = work out / energy in \\

\subsubsection{Supply/Demand}
Reducing demand and increasing supply can extend the amount of time we can use a given resource. In this case, a relatively small decrease in the rate of growth of consumption was nearly as impactful as the more improbable scenario of doubling the resource base. Accelerating rates of consumption mean that even very large resource stocks get consumed in short order.

\subsubsection{Loan guarantees}
A form of public policy support for energy technologies. They are essentially a promise by the government to pay off the debt obligation of power plant project in the event that there is a default with an investment in building and deploying a technology. Loan guarantees serve to shift the investment risk to the public sector, thereby making investments safer for private capital. The GAO estimates that about half of the loans guaranteed by the US Department of Energy go into default.

\subsubsection{Challenges for Renewables} 
The most prominent renewables (wind and solar) are intermittent power sources, meaning that wind turbines and PV panels have variable output depending on the amount of wind and sun available from moment to moment. Coal, on the other hand, is a baseload source of electricity, very constant, directly under human management, and ease to use when trying to maintain constant electricity quality (stable voltage, frequency, etc.). Also unlike coal plants, wind and solar installations are quite sensitive to where they are sited. China may not have a enough good locations to place renewable facilities. Even with an abundance of sites, the transmission grid might have to be extend to these sites, which presents an additional cost.

\subsubsection{Pro/Con "Green"}
(1) switching to electric vehiclesDisadvantages: more frequent need to maintenance (battery replacement),limited rangeAdvantages: electricity is abundant, refueling can be done at Best Buy stores Strategy: Use PV panels to charge cars to reduce carbon intensity of fuel\\(2) switching 1⁄2 of the fleet to hydrogen fuel cell vehiclesDisadvantages: H2 is not commonly available, vehicles currently expensive Advantages: Greater range before refueling is necessary,Strategy: Use fuel made from electricity and water rather than natural gas\\(3) reducing the number of house calls madeDisadvantages: Possible reduced customer satisfactionAdvantages: No need to purchase or store vehicles, faster response time Strategy: Impose a limit on the distance the Geek Squad will travel, cuttingout the very long trips

\subsubsection{Readings}
{\bf Lovins, Amory (1976) “Energy Strategy: The Road Not Taken”, Foreign Affairs, 55(1): 65 – 96.}Two, mutually-exclusive energy paths for US energy over 50 years: rapid development of centralized energy (“hard” path) vs. combination of energy efficiency and renewable energy (“soft” path).\\“Hard” path\\•	Goes with massive electrification (coal, gas and nuclear) and resulting impact on land (increased water temperatures because of waste heat, displaced populations because of mines, increased carbon emissions).\\•	Electricity is a very inefficient use of fuels, and has huge capital costs, using a major part of the country’s GDP for new plants installation.\\“Soft” path (or energy-income economy)\\•	Goes with social and technical changes. Efficiency is cheaper, faster, longer-lasting and safer than adding to energy supplies.\\•	Energy efficiency is much cheaper than additional supplies because energy savings often exceed investment needed for more efficiency.\\•	Another savings potential comes from cogeneration (electricity as by-product of industrial process steam).\\•	Limits are purely institutional (building codes, etc.), not technical nor economic, and end-use efficiency could be doubled by 2000.\\•	“Soft” technologies: flexible, resilient, sustainable and benign.\\•	Energy sources are matched in energy quality with end-use needs (no wasteful electricity for uses that don’t require electricity).\\•	Appropriate scale rather than centralized power generation, resulting in reduced overhead costs (and reduced distribution losses).\\•	Technologies include high- and low-temperature solar collectors, organic converters, and wind machines, coupled with scaled energy storage matching end-use needs. Energy storage is easier to implement in a soft path economy than in a hard path economy (electricity is hard to store).\\•	Lower, more stable operating costs and lower initial costs with more attractive return on capital (because of shorter construction times).\\•	Reduces environmental (carbon emissions) and economic (plant failure) risks while diversifying technical risk.\\•	Adapted to the rural and urban poor (readily available energy sources).Transitional technologies•	Efficient use of fossil fuels (essentially “clean” coal with fluidized bed).{\bf Goldemberg, J. (1996) Energy, Environment, and Development (Earthscan: London, UK), pages 11 – 37.}•	Lorenz curve: income distribution vs. population. 61\% of global population has 5\% of global income (low-income countries), 23\% have 13\% (middle-income countries) and 17\% have 82\% (high-income countries).\\•	GDP/capita is a poor metric in countries with high disparity of income.\\•	Fuelwood is the dominant source of energy in rural areas, and cooking is the most energy-intensive activity.\\•	Social indicators (life expectancy, infant mortality, literacy, fertility rate) are closely related to energy consumption. 1 TOE/capita appears like a barrier.\\•	Because of energy crises in the 1970s, industrialized countries have looked for ways to reduce energy consumption, hence a “delinking” between GDP growth and energy growth.•	Shortfall of fuelwood in developing countries, which cannot afford the foreign exchange needed to replace fuelwood by petroleum.\\•	Biomass burning for cooking by the poor has been identified by the World Health Organization as the major indoor air pollution health problem in the world today. Women and children are most continuously exposed.\\{\bf Kammen, D. M. and Dove, M. R. (1997) “The virtues of mundane science”, Environment, 39(6): 10–15, 38–41.}•	In quest towards sustainability, “mundane” scientific topics often overlooked, although main cause of worldwide problems (e.g. impacts of indoor air pollution in Subsaharan Africa).\\•	Importance of integrating engineering, scientific and social science research.\\•	Improved cookstoves: better efficiency, less pollution.\\•	Applied science is just as important as (and not competing with) basic science. However, funding of applied science has decreased after WWII, leading to a reduced number of researchers dealing with mundane science. and development work in particular.\\{\bf Hirsh, Richard (1999) Power Loss (MIT University Press: Cambridge, MA) Section I-II, Pages 1 - 88.}•	At the beginning of the 20th century, electric utility companies had become monopolies, because of centralized production and consolidation.\\•	At first, monopolies were accompanied by municipalization: city governments would purchase the assets of utility companies and operate them for the benefit of the citizens.\\•	Another model for controlling power companies: state regulation (control by regulatory commissions to ensure benefits of both “public utilities” and customers). Valid for monopolies that constitute a necessity for society.\\•	Wisconsin’s public utilities law was the first to be widely popular. Rate of return regulation.\\•	Utility consensus: idea that regulated monopolies was the most efficient way to manage the power sector. Utilities were working with (often actually influencing) regulators, but the public was satisfied because of constant improvements in production efficiency (better and better steam turbines, etc.) leading to lower prices. Utilities were satisfied because the states legitimized their positions as monopolies.\\•	Natural monopoly: “increasing returns”, or economies of scale.\\•	Regulatory bodies required that power be made available to the whole population (obligation to serve), resulting in massive investments, but also allowed private utilities to acquire private property needed to serve the public (eminent domain).•	Regulation reduced financial risk in the industry (end of franchise).•	Dominance of utility managers in the system. Emergence of electrical engineering programs in universities at the end of the 19th century. Huge propaganda efforts in favor of utilities after WW1. Financing of political campaigns to serve the utilities’ interests.•	Closing of the utility system: vertical integration leading to utility managers controlling all variables of the system.•	Public Utility Holding Company Act limited the way utility companies could be organized.•	Roosevelt established the government-run Tennessee Valley Authority, which sold electricity at lower rates than private utilities, and Congress established the Rural Electrification Administration in 1935 to bring power to Southern farmers.•	Shock to utilities in the 70s: technological stasis (no more cost reduction through increased efficiency and turbine size), energy crisis (increased costs and shortages leading to reduced consumption), and environmental movement.•	Other types of production emerged, which had been curtailed by utility monopolies (wind, gas turbines).\\•	1978: Public Utility Regulatory Policies Act (PURPA): eliminate wasteful use of electricity; promote production of power in non traditional ways (cogeneration, renewables); removed barriers of entry into the generation sector (non utilities able to produce electricity at same cost as utilities, and utilities have to buy power from independent producers, questioning the utilities consensus); changed electricity rates (from declining blocks to marginal cost).\\
{\bf Pacala, S., and Socolow, R. (2004) “Stabilization wedges: solving the climate problem for the next fifty years with current technologies”, Science, 305, 968 – 971.}•	Humanity already possesses the fundamental scientific, technical, and industrial know-how to solve the carbon and climate problem for the next half-century.•	A “wedge” represents an activity that reduces emissions to the atmosphere that starts at zero today and increases linearly until it accounts for 1 GtC/year of reduced carbon emissions in 50 years. 7 “wedges” are needed to stabilize emissions to today’s level.•	After stabilizing emissions for 50 years, one must decrease emissions significantly in the second half of the century, using revolutionary technologies that must start being investigated.•	15 wedges are proposed, including efficiency, fuel shift, CCS, nuclear and renewables.Hansen, J., Sato, M. and Ruedy, R. (2012) “Perception of climate change”, PNAS•	Trend towards higher average temperatures and higher number of climate anomalies (extremely hot summers), over larger land areas.•	Global warming also leads to heavier rainfalls and higher number of floods.And for an example of the arguments against, see:http://cliffmass.blogspot.com/2012/08/climate-distortion.html•	Heat extremes are actually caused by natural variability for ~ 90\%.{\bf Climate Distortion}This week, with great fanfare,  NASA scientist James Hansen and associates released a paper "The Perception of Climate Change"  in the journal PNAS that claims that recent heat waves and droughts were caused by human-induced climate change.  To quote their abstract:" It follows that we can state, with a high degree of confidence, that extreme anomalies such as those in Texas and Oklahoma in 2011 and Moscow in 2010 were a consequence of global warming because their likelihood in the absence of global warming was exceedingly small."This paper (found here) has been quoted in hundreds, if not thousands, of media outlets and newspapers and has garnered the praise of many environmental advocacy groups. The problem?  Their conclusions are demonstrably false and their characterization of the science and statistics are deceptive at best.  And the problem goes beyond this unfortunate paper.  It extends to the way the media has misunderstood and miscommunicated our current state of knowledge of climate change.  No wonder the public is confused, skeptic/denier groups hold on to wacky/unscientific theories, and our leaders dither on climate change.  And let me repeat something I have said several times....I believe that human-induced global warming is both observed, real, and a serious problem for mankind.  So if anyone wants to call me a denier or some other ad hominem name, please refrain from such remarks. Well, lets start with a little test.The heat waves/droughts in the mid-section of the U.S. during past two years were caused by:a.  90\% natural variability and 10\% human-induced global warmingb.  50\% natural variability and 50\% human-induced global warmingc.  10\% natural variability and 90\% human-induced global warmingTime is up!  Write down your answer.  As I will try to demonstrate, the correct answer is probably very close to (a).  90\% of the temperature anomaly this and last summer is the result of natural variability with a minor assist from global warming.Really different than the impression you are getting from Dr. Hansen and the media, right?  But it is the truth.Let me prove to you now that Dr. Hansen's claims are  deceptive.  Consider the heat wave in  Texas/Oklahoma last year.  Below you will find the mean temperatures for July and August over the U.S. (top panels), while below are the differences (anomalies) from normal (or climatology).  The anomalies were over 8F in July and over 7F in August.  How big could the global warming signal be?  And particularly the warming due to mankind's emission of greenhouse gases?  The IPCC is the world scientific body that has examined such questions.  They note that human influence should have become significant somewhere in the mid-70s and the generally accepted estimated of the warming of the Northern Hemisphere since then is roughly 1F or  C (see IPCC graphic below). Now this warming could well include some component of natural warming (particularly since there had been warming since the end of the 1800s).   But lets assume it is all due to human-emitted greenhouse gases. The full one degree F.But what about Texas and Oklahoma?  The hemispheric value includes the Arctic over when the IPCC has shown the warming is far larger than anywhere else (due to melting sea ice for one reason).   Here are the temperature changes over all of Texas for June to August.   A big spike in 2011...but what about the long-term trend?   ...that would be the trend associated with global warming.  Nearly flat, but perhaps we can convince ourselves there was a small upward trend since 1970 of perhaps .5-1F.   And remember these observations are contaminated by urban heat island effects. So I think you can see that the global warming signal due to human-emitted gases could not possibly be more than 1F, and is probably much less.  Yet the heat wave last summer, expressed as monthly anomalies, reached 7-8F over large portions of Texas and Oklahoma.What can you conclude?  Something other than global warming produced the lion's share of the heat wave...and we know what it was:  a major change in the circulation over the U.S. last summer.  A big area of high pressure and high heights over the center of the U.S. The graphic below shows the differences of the upper level (500hPa) heights from normal over the U.S.  for last summer.  Green indicates above normal heights (a ridge) and blues indicate lower heights (trough).   So last summer we had a persistent pattern of mid-continental ridging and troughing on the coast.  This summer has been the same.  The upper level flow pattern has amplified in a wave-like way. And there is no reason to think, based on theoretical or observational research that this is anything but natural variability.  To put it another way: in July of last year, at least 7/8th (87\%) of the warming was due to natural processes, and the truth surely was well over 90\%.  It is funny that Dr. Hansen and others exaggerating the effects of human emissions don't say it this way.  The correct way.The media, repeating this stuff, provides deceptive storiesy to their readers.  And the damage to the credibility of my profession is huge.As an aside, the journal that this article was published in...the Proceedings of the National Academy of Sciences (PNAS)...allows members of the National Academy (like Dr. Hansen) to publish articles with essentially no peer review.  Until 2010 they could publish anything, with no peer review, and most recently the submission review is "supervised" by the submitting academy member WHO GETS TO SELECT THE REVIEWERS. Folks, this is really unfortunate for an entity that claims to be national journal of some reputation.  The result has been a lot of very bad papers in PNAS that would never have been accepted in real journals,with a real peer review process.  One could use stronger words, but this is a family blog.So you think these exaggerations of Hansen and his fellow-travelers is bad enough?  It is much worse than that.   They spend a lot of time talking about statistics (bell curves) and how warming is producing more extremes.  But they don't give folks the straight story...let me explain.Many atmospheric and other variables follow a bell curve (a.k.a. a Gaussian) in which most observations are near the mean and then the frequency drops away towards extremes on both sides (see graphic).  So the probability is highest that the observation is near the mean and probabilities drop for higher and lower values.  An important aspect of the bell curve is the standard deviation (indicated by Greek letter sigma).  You can calculate this number from the observations (I won't go into that here, but it is straightforward and in all statistics texts).  Turns out the that if the observation type is well described by a bell curve (and temperature generally is), 68\% of the observations should fall within one standard deviation (one sigma) of the mean (both above and below).  The graphic shows this with the dark blue color.  Roughly 95\% should fall within two sigma of the mean (dark plus medium blue), and roughly 99\% within 3 sigma.  So very few observations..the most extreme should fall more than 3 standard deviations from the mean.  Now as the earth warms up the temperature variations shown remain like the bell curve...or Gaussian, but the mean should shift to warmer temperatures (see the figure below). The result is that you get more warm extremes and less cold extremes (less cold extremes are not mentioned very often for some reason). So the result is that you seem more warm temperature records and less cold temperature records.   We are in fact seeing this.  The earth is warming and there are more maximum temperature records than cold ones.  Hansen and friends make a big deal about this.But what they are not telling you is that the very warm anomalies we are seeing today would have been nearly as large if global warming had never occurred.  In his paper he makes a big deal about large (three sigma) anomalies from climatology.   Well, without any global warming the anomalies might have been say 2.8 sigma.   Or in terms of terms, heat waves of 10F might have beenonly 9F if global warming had not occurred.  To say it differently, the impact of global warming due to greenhouse gases is still small compared to natural processes, and the impacts to society would have been pretty much the same.  But you never hear it this way.   Those exaggerating the global warming signal imply that we are going from normal conditions to extremes due to global warming.  In reality, we go from naturally induced extremes, to a bit stronger extremes due to global warming.Let me say it am alternative way. You can see from the bell curve that the probability of a certain temperature changes rapidly when you are on the sides of the bell curve.  So if the bell curve shifts warmer, you get a big change in the probabilities of high temperatures.  So you get a lot more records even if the warming...and the impacts of the warming...are not that large.Lets be clear here.  Although the global warming signal is relatively weak today in most of the planet (outside of the Arctic), our best science indicates that the warming will greatly increase by the end of the century.  At this point, the human-induced warming will be much more comparable to the natural variability and our climate will truly be different.Unfortunately, a very limited, but highly visible, group of scientists like Hansen are choosing to tell a story that is not supported by the facts, with a media that is happy to amplify such claims.  Global warming due to greenhouse gas emissions of mankind is a very serious issue...one which our civilization is not dealing with in an effective way.  But scientists must give society the straight facts and not shade or exaggerate the facts based on our personal views on what should be done. }

% You can even have references
\rule{0.3\linewidth}{0.25pt}
\scriptsize
\bibliographystyle{abstract}
\bibliography{refFile}
\end{multicols}
\end{document}